{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge, ElasticNet \n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier, RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier, \\\n",
    "ExtraTreesRegressor, ExtraTreesClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import mean_squared_error, f1_score, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./401ksubs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9275, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>0</td>\n",
       "      <td>58.428</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3413.8310</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>0</td>\n",
       "      <td>24.546</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>602.5061</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9272</th>\n",
       "      <td>0</td>\n",
       "      <td>38.550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>-13.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1486.1020</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>0</td>\n",
       "      <td>34.410</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1184.0480</td>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9274</th>\n",
       "      <td>0</td>\n",
       "      <td>25.608</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>655.7697</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      e401k     inc  marr  male  age  fsize  nettfa  p401k  pira      incsq  \\\n",
       "9270      0  58.428     1     0   33      4   -1.20      0     0  3413.8310   \n",
       "9271      0  24.546     0     1   37      3    2.00      0     0   602.5061   \n",
       "9272      0  38.550     1     0   33      3  -13.60      0     1  1486.1020   \n",
       "9273      0  34.410     1     0   57      3    3.55      0     0  1184.0480   \n",
       "9274      0  25.608     0     1   49      1    1.80      0     0   655.7697   \n",
       "\n",
       "      agesq  \n",
       "9270   1089  \n",
       "9271   1369  \n",
       "9272   1089  \n",
       "9273   3249  \n",
       "9274   2401  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check last 5 rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8172</th>\n",
       "      <td>0</td>\n",
       "      <td>13.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.6336</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8304</th>\n",
       "      <td>0</td>\n",
       "      <td>13.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.6336</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      e401k    inc  marr  male  age  fsize  nettfa  p401k  pira     incsq  \\\n",
       "8172      0  13.44     0     0   42      4     0.0      0     0  180.6336   \n",
       "8304      0  13.44     0     0   42      4     0.0      0     0  180.6336   \n",
       "\n",
       "      agesq  \n",
       "8172   1764  \n",
       "8304   1764  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index 8304 is a duplicate of 8172\n",
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate row\n",
    "df.drop(8304, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k       int64\n",
       "inc       float64\n",
       "marr        int64\n",
       "male        int64\n",
       "age         int64\n",
       "fsize       int64\n",
       "nettfa    float64\n",
       "p401k       int64\n",
       "pira        int64\n",
       "incsq     float64\n",
       "agesq       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fsize\n",
       "2     2199\n",
       "1     2017\n",
       "4     1989\n",
       "3     1829\n",
       "5      816\n",
       "6      268\n",
       "7       95\n",
       "8       38\n",
       "10       7\n",
       "9        7\n",
       "12       4\n",
       "11       3\n",
       "13       2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check fsize value counts, will need to OHE for modeling\n",
    "df['fsize'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Whether or not person has a college degree\n",
    "2. Whether or not person has children\n",
    "3. Would be helpful to have variables that predict financial stability:\n",
    "   - a) how much money is in one's savings account\n",
    "   - b) how much money is in one's checking (American terminology) account\n",
    "   - c) what is one's credit score (does anyone in Singapore care?)\n",
    "   - d) owning a house versus renting a residence may be a good indicator of someone able and willing to save money. (The same goes for a car)\n",
    "   - e) knowing whether or not the person has made investments into stocks, bonds, or Certificates of Deposits (CDs) would also be helpful to predict eligibility for 401(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting race into a model is likely an unethical thing to do if you aren't in China (jokes?). In this particular scenario, using race to predict whether or not someone is eligible for a 401k is using race to target who should qualify for a particular product or service. It is unacceptable to discriminate on the basis of race (I suppose, depends on the country), even if race by itself wouldn't immediately disqualify somebody from being targeted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wouldn't use income to predict income - because we wouldn't need to predict something if we already possessed the data we wanted to predict. Similarly, if we had access to incsq, then we wouldn't need to predict inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'incsq' and 'agesq' appear to already have been feature engineered. Assuming that this was done by subject-matter experts, it's likely that they found there's some quadratic (squared) relationship between income and eligibility for a 401k. (The same goes for age and eligibility for a 401k.) Perhaps people who are older or have higher incomes are exponentially more likely to be eligible for an IRA, so these terms account for that nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "There appear to be two errors. I'm not sure if this is deliberate. 'inc' is defined as inc^2 and age is defined as age^2. While these are correct for incsq and agesq, inc should refer to one's income (not sure if it's household or individual, but probably individual) and age should refer to one's age. The data dictionary also does not mention that the income is in 1000s, although this is mentioned for net total financial assets (nettfa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Regression: An appropriate tactic for predicting one's income and coefficients can be interpreted fairly easily. Unit change in x-variable leads to change in y-variable multiplied by the coefficient of that x-variable\n",
    "- Ridge Regression: Similar to non-regularized linear regression, but coefficients have been penalised equivalent to the sum of the squares of the magnitude of coefficients\n",
    "- Lasso Regression: Similar to non-regularized linear regression, but coefficients have been penalised equivalent to the sum of the absolute values of coefficients\n",
    "- ElasticNet Regression: Combines the effects of Lasso and Ridge regularization\n",
    "- k-nearest neighbors model: Does not provide a prediction for the importance or coefficients of variables\n",
    "- A decision tree: A single tree is interpretable. Humans can visualize and understand a tree, no matter if they're machine learning experts or laypeople\n",
    "- A set of bagged decision trees: Bootstrap aggregate of many decision tree. Not easy to interpret compared to a single decision tree\n",
    "- A random forest: Bootstrap aggregate many decision trees and selects random subspace of features. Not easy to interpret compared to a single decision tree\n",
    "- A set of randomized trees: Similar to random forest, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. Not easy to interpret compared to a single decision tree\n",
    "- An Adaboost model: Meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases. Can select base estimator, but if decision tree regressor, then not easy to interpret\n",
    "- A gradient boosted model: Gradient Boosting is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function. Not easy to interpret compared to a single decision tree\n",
    "- A support vector regressor: Inspect .support_vectors_, though these are hard to interpret. With linear kernels, you can inspect .coef_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom scorer for regression models that prints mean cross validated RMSE score and train/test RMSE/R2 score\n",
    "def custom_scorer(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f'For the model {model}:', '\\n')\n",
    "    print(f'The best parameters as chosen by GridSearchCV is {model.best_params_}')\n",
    "    print(f'The mean cross-validated RMSE score of the best_estimator is {-model.best_score_}')\n",
    "    print(f'The train R2 score is {r2_score(y_train, model.predict(X_train))}')\n",
    "    print(f'The train RMSE score is {-model.score(X_train, y_train)}')\n",
    "    print(f'The test R2 score is {r2_score(y_test, model.predict(X_test))}')\n",
    "    print(f'The test RMSE score is {-model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns to create feature matrix\n",
    "X = df.drop(columns = ['e401k', 'p401k', 'pira', 'inc', 'incsq'])\n",
    "# Create target matrix\n",
    "y = df['inc']\n",
    "# Train/test (80/20) split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 preprocessors, one for regression models that require feature scaling and\n",
    "# another one for regression models that do not require feature scaling\n",
    "\n",
    "# Scale numeric columns and OHE categorical columns\n",
    "ct_ss = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scale\", StandardScaler(), ['age', 'nettfa', 'agesq']),\n",
    "        (\"ohe\", OneHotEncoder(drop='first'), ['marr', 'male', 'fsize'])\n",
    "    ],\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# OHE categorical columns\n",
    "ct_no_ss = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ohe\", OneHotEncoder(drop='first'), ['marr', 'male', 'fsize'])\n",
    "    ],\n",
    "    n_jobs = -1,\n",
    "     remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross-validated RMSE score is 20.314919217166963\n",
      "The train R2 score is 0.2969602497608077\n",
      "The train RMSE score is 20.237568545903724\n",
      "The test R2 score is 0.2586329552548243\n",
      "The test RMSE score is 20.575900557907673\n"
     ]
    }
   ],
   "source": [
    "# For linear regression model\n",
    "lr = make_pipeline(ct_no_ss, LinearRegression())\n",
    "print(f'The mean cross-validated RMSE score is {-np.mean(cross_val_score(lr, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1))}')\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr_y_train_pred = lr.predict(X_train)\n",
    "lr_y_test_pred = lr.predict(X_test)\n",
    "\n",
    "print(f'The train R2 score is {lr.score(X_train, y_train)}')\n",
    "print(f'The train RMSE score is {mean_squared_error(y_train, lr_y_train_pred, squared=False)}')\n",
    "print(f'The test R2 score is {lr.score(X_test, y_test)}')\n",
    "print(f'The test RMSE score is {mean_squared_error(y_test, lr_y_test_pred, squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          transformers=[('scale',\n",
      "                                                                         StandardScaler(),\n",
      "                                                                         ['age',\n",
      "                                                                          'nettfa',\n",
      "                                                                          'agesq']),\n",
      "                                                                        ('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('lasso', Lasso(random_state=42))]),\n",
      "             param_grid={'lasso__alpha': array([0.001     , 0.00102353, 0.00104762, 0.00107227, 0.0010975 ,\n",
      "       0.00112332...\n",
      "       0.00509414, 0.00521401, 0.0053367 , 0.00546228, 0.00559081,\n",
      "       0.00572237, 0.00585702, 0.00599484, 0.00613591, 0.00628029,\n",
      "       0.00642807, 0.00657933, 0.00673415, 0.00689261, 0.0070548 ,\n",
      "       0.00722081, 0.00739072, 0.00756463, 0.00774264, 0.00792483,\n",
      "       0.00811131, 0.00830218, 0.00849753, 0.00869749, 0.00890215,\n",
      "       0.00911163, 0.00932603, 0.00954548, 0.0097701 , 0.01      ])},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'lasso__alpha': 0.004977023564332114}\n",
      "The mean cross-validated RMSE score of the best_estimator is 20.30571833691622\n",
      "The train R2 score is 0.297213106024491\n",
      "The train RMSE score is 20.23392888245365\n",
      "The test R2 score is 0.2590935496654966\n",
      "The test RMSE score is 20.56950789633772\n"
     ]
    }
   ],
   "source": [
    "# For LASSO linear regression model\n",
    "lasso_alphas = np.logspace(-3, -2, 100)\n",
    "lasso_r = make_pipeline(ct_ss, Lasso(random_state=42))\n",
    "param_grid_lasso_r = {\"lasso__alpha\": lasso_alphas,\n",
    "                     }\n",
    "grid_lasso_r = GridSearchCV(lasso_r, param_grid_lasso_r, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_lasso_r, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          transformers=[('scale',\n",
      "                                                                         StandardScaler(),\n",
      "                                                                         ['age',\n",
      "                                                                          'nettfa',\n",
      "                                                                          'agesq']),\n",
      "                                                                        ('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('ridge', Ridge(random_state=42))]),\n",
      "             param_grid={'ridge__alpha': array([0.1       , 0.1023531 , 0.10476158, 0.10722672, 0.10974988,\n",
      "       0.1123324 , 0....\n",
      "       0.5094138 , 0.52140083, 0.53366992, 0.54622772, 0.55908102,\n",
      "       0.57223677, 0.58570208, 0.59948425, 0.61359073, 0.62802914,\n",
      "       0.64280731, 0.65793322, 0.67341507, 0.68926121, 0.70548023,\n",
      "       0.7220809 , 0.7390722 , 0.75646333, 0.77426368, 0.7924829 ,\n",
      "       0.81113083, 0.83021757, 0.84975344, 0.869749  , 0.89021509,\n",
      "       0.91116276, 0.93260335, 0.95454846, 0.97700996, 1.        ])},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'ridge__alpha': 1.0}\n",
      "The mean cross-validated RMSE score of the best_estimator is 20.30734636828682\n",
      "The train R2 score is 0.297541324998422\n",
      "The train RMSE score is 20.2292034564311\n",
      "The test R2 score is 0.2597818343199543\n",
      "The test RMSE score is 20.559951382020937\n"
     ]
    }
   ],
   "source": [
    "# For Ridge linear regression model\n",
    "ridge_alphas = np.logspace(-1, 0, 100)\n",
    "ridge_r = make_pipeline(ct_ss, Ridge(random_state=42))\n",
    "param_grid_ridge_r = {\"ridge__alpha\": ridge_alphas,\n",
    "                     }\n",
    "grid_ridge_r = GridSearchCV(ridge_r, param_grid_ridge_r, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_ridge_r, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          transformers=[('scale',\n",
      "                                                                         StandardScaler(),\n",
      "                                                                         ['age',\n",
      "                                                                          'nettfa',\n",
      "                                                                          'agesq']),\n",
      "                                                                        ('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('elasticnet',\n",
      "                                        ElasticNet(random_state=42))]),\n",
      "             param_grid={'elasticnet__alpha': array([0.001     , 0.00102353, 0.00104762, 0.00107227, 0.001...\n",
      "       0.00572237, 0.00585702, 0.00599484, 0.00613591, 0.00628029,\n",
      "       0.00642807, 0.00657933, 0.00673415, 0.00689261, 0.0070548 ,\n",
      "       0.00722081, 0.00739072, 0.00756463, 0.00774264, 0.00792483,\n",
      "       0.00811131, 0.00830218, 0.00849753, 0.00869749, 0.00890215,\n",
      "       0.00911163, 0.00932603, 0.00954548, 0.0097701 , 0.01      ]),\n",
      "                         'elasticnet__l1_ratio': array([0.5   , 0.6225, 0.745 , 0.8675, 0.99  ])},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'elasticnet__alpha': 0.003944206059437656, 'elasticnet__l1_ratio': 0.99}\n",
      "The mean cross-validated RMSE score of the best_estimator is 20.305580810060597\n",
      "The train R2 score is 0.29726842448491997\n",
      "The train RMSE score is 20.233132530223177\n",
      "The test R2 score is 0.2591232727213244\n",
      "The test RMSE score is 20.569095297174087\n"
     ]
    }
   ],
   "source": [
    "# For ElasticNet linear regression model\n",
    "enet_alphas = np.logspace(-3, -2, 100)\n",
    "enet_ratio = np.linspace(0.5, 0.99, 5)\n",
    "enet = make_pipeline(ct_ss, ElasticNet(random_state=42))\n",
    "param_grid_enet = {\"elasticnet__alpha\": enet_alphas,\n",
    "                   \"elasticnet__l1_ratio\": enet_ratio,\n",
    "                  }\n",
    "grid_enet = GridSearchCV(enet, param_grid_enet, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_enet, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          transformers=[('scale',\n",
      "                                                                         StandardScaler(),\n",
      "                                                                         ['age',\n",
      "                                                                          'nettfa',\n",
      "                                                                          'agesq']),\n",
      "                                                                        ('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('kneighborsregressor',\n",
      "                                        KNeighborsRegressor(n_jobs=-1))]),\n",
      "             param_grid={'kneighborsregressor__n_neighbors': [2, 3, 4, 5, 6, 7,\n",
      "                                                              8, 9, 10],\n",
      "                         'kneighborsregressor__p': [1, 2]},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'kneighborsregressor__n_neighbors': 10, 'kneighborsregressor__p': 2}\n",
      "The mean cross-validated RMSE score of the best_estimator is 19.578095793122593\n",
      "The train R2 score is 0.46340367943690797\n",
      "The train RMSE score is 17.680406156778073\n",
      "The test R2 score is 0.30985046510422853\n",
      "The test RMSE score is 19.8524364158787\n"
     ]
    }
   ],
   "source": [
    "# For KNN regression model\n",
    "knnr = make_pipeline(ct_ss, KNeighborsRegressor(n_jobs=-1))\n",
    "param_grid_knnr = {\"kneighborsregressor__n_neighbors\": [2,3,4,5,6,7,8,9,10],\n",
    "                   \"kneighborsregressor__p\": [1,2],\n",
    "                  }\n",
    "grid_knnr = GridSearchCV(knnr, param_grid_knnr, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_knnr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('decisiontreeregressor',\n",
      "                                        DecisionTreeRegressor())]),\n",
      "             param_grid={'decisiontreeregressor__criterion': ['squared_error',\n",
      "                                                              'friedman_mse',\n",
      "                                                              'absolute_error',\n",
      "                                                              'poisson'],\n",
      "                         'decisiontreeregressor__max_depth': [2, 3, 4, 5, 6]},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'decisiontreeregressor__criterion': 'poisson', 'decisiontreeregressor__max_depth': 5}\n",
      "The mean cross-validated RMSE score of the best_estimator is 18.93158403220617\n",
      "The train R2 score is 0.42003807484333566\n",
      "The train RMSE score is 18.380957617932463\n",
      "The test R2 score is 0.38240232808924335\n",
      "The test RMSE score is 18.779976263290198\n"
     ]
    }
   ],
   "source": [
    "# For Decision Tree regression model\n",
    "dtr = make_pipeline(ct_no_ss, DecisionTreeRegressor())\n",
    "param_grid_dtr = {\"decisiontreeregressor__criterion\": ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "                  \"decisiontreeregressor__max_depth\": [2,3,4,5,6],\n",
    "                 }\n",
    "grid_dtr = GridSearchCV(dtr, param_grid_dtr, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_dtr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('baggingregressor',\n",
      "                                        BaggingRegressor(n_jobs=-1,\n",
      "                                                         random_state=42))]),\n",
      "             param_grid={'baggingregressor__n_estimators': [10, 15, 20, 25]},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'baggingregressor__n_estimators': 25}\n",
      "The mean cross-validated RMSE score of the best_estimator is 20.19043724033258\n",
      "The train R2 score is 0.886892994318979\n",
      "The train RMSE score is 8.117328932716529\n",
      "The test R2 score is 0.2495842847562253\n",
      "The test RMSE score is 20.701088123867805\n"
     ]
    }
   ],
   "source": [
    "# For Bagged Decision Tree regression model\n",
    "br = make_pipeline(ct_no_ss, BaggingRegressor(n_jobs=-1, random_state=42))\n",
    "param_grid_br = {\"baggingregressor__n_estimators\": [10, 15, 20, 25],\n",
    "                }\n",
    "grid_br = GridSearchCV(br, param_grid_br, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_br, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('extratreesregressor',\n",
      "                                        ExtraTreesRegressor(n_jobs=-1,\n",
      "                                                            random_state=42))]),\n",
      "             param_grid={'extratreesregressor__criterion': ['squared_error',\n",
      "                                                            'friedman_mse',\n",
      "                                                            'absolute_error',\n",
      "                                                            'poisson'],\n",
      "                         'extratreesregressor__max_depth': [2, 3, 4],\n",
      "                         'extratreesregressor__n_estimators': [75, 100, 125,\n",
      "                                                               150]},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'extratreesregressor__criterion': 'friedman_mse', 'extratreesregressor__max_depth': 4, 'extratreesregressor__n_estimators': 125}\n",
      "The mean cross-validated RMSE score of the best_estimator is 20.790407381886205\n",
      "The train R2 score is 0.264623634293385\n",
      "The train RMSE score is 20.697754254655273\n",
      "The test R2 score is 0.24685460548887805\n",
      "The test RMSE score is 20.738704631637763\n"
     ]
    }
   ],
   "source": [
    "# For Extra Trees regression model\n",
    "etr = make_pipeline(ct_no_ss, ExtraTreesRegressor(n_jobs=-1, random_state=42))\n",
    "param_grid_etr = {\"extratreesregressor__n_estimators\": [75, 100, 125, 150],\n",
    "                  \"extratreesregressor__criterion\": ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "                  \"extratreesregressor__max_depth\": [2,3,4],\n",
    "                 }\n",
    "grid_etr = GridSearchCV(etr, param_grid_etr, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_etr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('randomforestregressor',\n",
      "                                        RandomForestRegressor(n_jobs=-1,\n",
      "                                                              random_state=42))]),\n",
      "             param_grid={'randomforestregressor__criterion': ['squared_error',\n",
      "                                                              'friedman_mse',\n",
      "                                                              'absolute_error',\n",
      "                                                              'poisson'],\n",
      "                         'randomforestregressor__max_depth': [2, 3, 4],\n",
      "                         'randomforestregressor__n_estimators': [75, 100, 125,\n",
      "                                                                 150]},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'randomforestregressor__criterion': 'poisson', 'randomforestregressor__max_depth': 4, 'randomforestregressor__n_estimators': 75}\n",
      "The mean cross-validated RMSE score of the best_estimator is 18.794363569440133\n",
      "The train R2 score is 0.41317971484366056\n",
      "The train RMSE score is 18.489320525261373\n",
      "The test R2 score is 0.3856329780030543\n",
      "The test RMSE score is 18.730792884880966\n"
     ]
    }
   ],
   "source": [
    "# For Random Forest regression model\n",
    "rfr = make_pipeline(ct_no_ss, RandomForestRegressor(n_jobs=-1, random_state=42))\n",
    "param_grid_rfr = {\"randomforestregressor__criterion\": ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "                  \"randomforestregressor__max_depth\": [2,3,4],\n",
    "                  \"randomforestregressor__n_estimators\": [75, 100, 125, 150],\n",
    "                 }\n",
    "grid_rfr = GridSearchCV(rfr, param_grid_rfr, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_rfr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('adaboostregressor',\n",
      "                                        AdaBoostRegressor())]),\n",
      "             param_grid={'adaboostregressor__loss': ['linear', 'square',\n",
      "                                                     'exponential'],\n",
      "                         'adaboostregressor__n_estimators': [75, 100, 125,\n",
      "                                                             150]},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'adaboostregressor__loss': 'linear', 'adaboostregressor__n_estimators': 150}\n",
      "The mean cross-validated RMSE score of the best_estimator is 22.115978509968652\n",
      "The train R2 score is 0.1659042988500109\n",
      "The train RMSE score is 22.043285912933094\n",
      "The test R2 score is 0.11182055496346954\n",
      "The test RMSE score is 22.521254222555907\n"
     ]
    }
   ],
   "source": [
    "# For AdaBoost regression model\n",
    "adar = make_pipeline(ct_no_ss, AdaBoostRegressor())\n",
    "param_grid_adar = {\"adaboostregressor__n_estimators\": [75, 100, 125, 150],\n",
    "                   \"adaboostregressor__loss\": ['linear', 'square', 'exponential'],\n",
    "                  }\n",
    "grid_adar = GridSearchCV(adar, param_grid_adar, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_adar, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('gradientboostingregressor',\n",
      "                                        GradientBoostingRegressor(random_state=42))]),\n",
      "             param_grid={'gradientboostingregressor__criterion': ['friedman_mse',\n",
      "                                                                  'squared_error'],\n",
      "                         'gradientboostingregressor__loss': ['squared_error',\n",
      "                                                             'absolute_error',\n",
      "                                                             'huber',\n",
      "                                                             'quantile'],\n",
      "                         'gradientboostingregressor__max_depth': [2, 3],\n",
      "                         'gradientboostingregressor__n_estimators': [75, 100,\n",
      "                                                                     125,\n",
      "                                                                     150]},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'gradientboostingregressor__criterion': 'squared_error', 'gradientboostingregressor__loss': 'squared_error', 'gradientboostingregressor__max_depth': 3, 'gradientboostingregressor__n_estimators': 100}\n",
      "The mean cross-validated RMSE score of the best_estimator is 18.57511880615893\n",
      "The train R2 score is 0.4595386452729717\n",
      "The train RMSE score is 17.74396675128108\n",
      "The test R2 score is 0.3941328926741977\n",
      "The test RMSE score is 18.60076909148151\n"
     ]
    }
   ],
   "source": [
    "# For Gradient Boosted regression model\n",
    "gbr = make_pipeline(ct_no_ss, GradientBoostingRegressor(random_state=42))\n",
    "param_grid_gbr = {\"gradientboostingregressor__loss\": ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "                  \"gradientboostingregressor__n_estimators\": [75, 100, 125, 150],\n",
    "                  \"gradientboostingregressor__criterion\": ['friedman_mse', 'squared_error'],\n",
    "                  \"gradientboostingregressor__max_depth\": [2, 3],\n",
    "                 }\n",
    "grid_gbr = GridSearchCV(gbr, param_grid_gbr, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_gbr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          transformers=[('scale',\n",
      "                                                                         StandardScaler(),\n",
      "                                                                         ['age',\n",
      "                                                                          'nettfa',\n",
      "                                                                          'agesq']),\n",
      "                                                                        ('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize'])])),\n",
      "                                       ('svr', SVR())]),\n",
      "             param_grid={'svr__degree': [2, 3, 4],\n",
      "                         'svr__gamma': ['scale', 'auto'],\n",
      "                         'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
      "             scoring='neg_root_mean_squared_error'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'svr__degree': 2, 'svr__gamma': 'scale', 'svr__kernel': 'rbf'}\n",
      "The mean cross-validated RMSE score of the best_estimator is 20.064857469012637\n",
      "The train R2 score is 0.32171038944593966\n",
      "The train RMSE score is 19.878150517376742\n",
      "The test R2 score is 0.32583639670219144\n",
      "The test RMSE score is 19.621168392096877\n"
     ]
    }
   ],
   "source": [
    "# For Support Vector regression model\n",
    "svr = make_pipeline(ct_ss, SVR())\n",
    "param_grid_svr = {\"svr__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                  \"svr__degree\": [2,3,4,],\n",
    "                  \"svr__gamma\": ['scale', 'auto'],\n",
    "                 }\n",
    "grid_svr = GridSearchCV(svr, param_grid_svr, scoring='neg_root_mean_squared_error', cv=5)\n",
    "custom_scorer(grid_svr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Bootstrapping is a method of sampling with replacement. We usually use it in order to simulate many different samples or to empirically estimate the sampling distribution of a statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a set of bagged decision trees, we have bootstrapped (iteratively taking a random sample of the rows with replacement in a dataset) different samples and grown one decision tree on each bootstrapped sample, then our predictions are aggregated. A set of bagged decision trees is an ensemble method, meant to make 'weak signals' stronger, reducing variance in the model. With one decision tree, we only use the original sample and grow exactly one decision tree and no aggregation of predictions occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The difference between a set of bagged decision trees and a random forest is that a random subset of features is selected when building each tree (the best split is found from a random subset of the max features) in a random forest model. In bagged decision trees, every feature is considered as a \"candidate\" for splitting at each node in the individual decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "A random forest randomly selects which features go into each split. This effectively makes our individual decision trees less correlated when compared to the individual trees that select all the features for each split in a bagged decision trees model. This combination of diverse trees decreases the variance of our predictions after aggregation of the different decision trees, although sometimes at the cost of a slight increase in bias. Thus, a random forest usually has less variance (less overfitting) compared to a set of bagged decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|               Model              | Training RMSE | Cross-validated RMSE | Testing RMSE |\r\n",
    "|:--------------------------------:|:-------------:|:--------------------:|:------------:|\r\n",
    "|         Linear Regression        |    20.2815    |        20.3562       |    20.4585   |\r\n",
    "|      Lasso Linear Regression     |    20.2639    |        20.3458       |    20.4515   |\r\n",
    "|      Ridge Linear Regression     |    20.2614    |        20.3458       |    20.4483   |\r\n",
    "|   ElasticNet Linear Regression   |    20.2628    |        20.3456       |    20.4515   |\r\n",
    "|  k-Nearest Neighbors Regression  |    17.7471    |        19.6855       |    19.5486   |\r\n",
    "|     Decision Tree Regression     |    18.4067    |        18.8583       |    18.6100   |\r\n",
    "| Bagged Decision Trees Regression |     8.1597    |        20.5272       |    20.2592   |\r\n",
    "|      Extra Trees Regression      |    20.7467    |        20.7777       |    20.4931   |\r\n",
    "|     Random Forest Regression     |    18.5145    |        18.8169       |    18.6040   |\r\n",
    "|        AdaBoost Regression       |    22.7004    |        21.8013       |    23.1081   |\r\n",
    "|    Gradient Boosted Regression   |    17.9361    |        18.6004       |    18.2975   |\r\n",
    "|     Support Vector Regression    |    19.8861    |        20.0725       |    19.6709   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training RMSE and the testing RMSE, there is evidence of more overfitting, in this order:\n",
    "\n",
    "1. Bagged Decision Trees Regression: Difference in RMSE of about 12\n",
    "2. k-Nearest Neighbors Regression: Difference in RMSE of about 1.8\n",
    "3. AdaBoost Regression: Difference in RMSE of about 0.4\n",
    "4. The rest have differences in RMSE of 0.3 or less\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I have a series of models from which I can pick, I usually do the following:\n",
    "\n",
    "1. List out all of the models. Any model that cannot solve my problem should be removed. My problem statement is \"What features best predict one's income?\" Even if it's a very predictive model, my goal isn't just to come up with the best predictions. I want to find the model that performs the best based on a metric of my choice, in this case the RMSE score and also has coefficients that can be interpreted, so as to answer the problem statement\n",
    "2. In this case, the model with the best test RMSE is the **Gradient Boosted Regression**, although it is slightly more overfitted than the Random Forest Regression, as based on difference in train/test RMSE. Regarding interpretability of my selected model:\n",
    "    - Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the impurity-based feature importance of each tree. I have calculated the regression coefficients for my selected model in the next code cell. The output will be displayed at the very end of the notebook when answering the final question\n",
    "3. Sometimes, model selection becomes a judgment call with no perfect guide to make the final decision.\n",
    "    - Do you have time to tune the models to try and eke out better performance?\n",
    "    - Is one model substantially better at solving the problem you wanted to solve?\n",
    "    - Do you need something understandable by a lay audience? (i.e. Linear regression is more common and more easily understood than AdaBoost or Support Vector Machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting coefficient values for our chosen regression model\n",
    "grid_gbr.fit(X_train, y_train) \n",
    "gbr.set_params(**grid_gbr.best_params_).fit(X_train, y_train)\n",
    "feature_values = gbr['gradientboostingregressor'].feature_importances_\n",
    "feature_names = gbr.named_steps.columntransformer.get_feature_names_out()\n",
    "regression_coefficients = pd.DataFrame(feature_values, index=feature_names).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I would look at higher-order terms. For example, since we have incsq and agesq automatically created for us, there may be other higher-order terms that could be predictive. I would explore these to see if we can make better predictions\n",
    "2. I would also consider interaction terms between variables. For example between income and nettfa\n",
    "3. I would consider transforming my variable. Income is often skewed, which means there will usually be a handful of very high incomes that might skew any linear model. I would consider transforming income (likely using log) so that income is \"un-skewed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Given that our target variable is whether or not someone is eligible for a 401k, then the variable 'p401k', for whether or not someone currently has a 401k leaks information about our target variable. Every person with a 401k must by definition be eligible for a 401k although a person without a 401k may or may not be eligible for a 401k. Including 'p401k' in my model would almost be like training the model with the target variable included as a feature, which, of course, would not lead to great results due to data leakage. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the model being constructed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A logistic regression model Yes, we can predict whether or not one is eligible for a 401(k)\n",
    "- k-nearest neighbors model: Yes, we can predict whether or not one is eligible for a 401(k)\n",
    "- A Naive Bayes model Yes, we can predict whether or not one is eligible for a 401(k)\n",
    "- A decision tree: Yes, we can predict whether or not one is eligible for a 401(k)\n",
    "- A set of bagged decision trees: Yes, we can predict whether or not one is eligible for a 401(k)\n",
    "- A random forest: Yes, we can predict whether or not one is eligible for a 401(k)\n",
    "- A set of randomized trees: Yes, we can predict whether or not one is eligible for a 401(k)\n",
    "- An Adaboost model: Yes, we can predict whether or not one is eligible for a 401(k)\n",
    "- A gradient boosted model: Yes, we can predict whether or not one is eligible for a 401(k)\n",
    "- A support vector classifier: Yes, we can predict whether or not one is eligible for a 401(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom scorer for classification models that prints mean cross validated F1 score and train/test F1 score\n",
    "def custom_scorer_classifier(model, X_train1, y_train1, X_test1, y_test1):\n",
    "    model.fit(X_train1, y_train1)\n",
    "    print(f'For the model {model}:', '\\n')\n",
    "    print(f'The best parameters as chosen by GridSearchCV is {model.best_params_}')\n",
    "    print(f'The mean cross-validated F1 score of the best_estimator is {model.best_score_}')\n",
    "    print(f'The train F1 score is {model.score(X_train1, y_train1)}')\n",
    "    print(f'The test F1 score is {model.score(X_test1, y_test1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns to create feature matrix\n",
    "X1 = df.drop(columns = ['e401k', 'p401k'])\n",
    "# Create target matrix\n",
    "y1 = df['e401k']\n",
    "# Train/test (80/20) split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1,\n",
    "                                                        y1,\n",
    "                                                        test_size = .2,\n",
    "                                                        random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 preprocessors, one for regression models that require feature scaling and\n",
    "# another one for regression models that do not require feature scaling\n",
    "\n",
    "# Scale numeric columns and OHE categorical columns\n",
    "ct_ss1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scale\", StandardScaler(), ['inc', 'age', 'nettfa', 'incsq', 'agesq']),\n",
    "        (\"ohe\", OneHotEncoder(drop='first'), ['marr', 'male', 'fsize', 'pira'])\n",
    "    ],\n",
    "    n_jobs = -1\n",
    ")\n",
    "# OHE categorical columns\n",
    "ct_no_ss1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ohe\", OneHotEncoder(drop='first'), ['marr', 'male', 'fsize', 'pira'])\n",
    "    ],\n",
    "    n_jobs = -1,\n",
    "     remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          transformers=[('scale',\n",
      "                                                                         StandardScaler(),\n",
      "                                                                         ['inc',\n",
      "                                                                          'age',\n",
      "                                                                          'nettfa',\n",
      "                                                                          'incsq',\n",
      "                                                                          'agesq']),\n",
      "                                                                        ('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('logisticregression',\n",
      "                                        LogisticRegression(max_iter=1000))]),\n",
      "             param_grid={'logisticregression__C': [0.01, 0.05, 0.1, 0.5, 1, 5,\n",
      "                                                   10],\n",
      "                         'logisticregression__penalty': ['l1', 'l2'],\n",
      "                         'logisticregression__solver': ['liblinear']},\n",
      "             scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'logisticregression__C': 1, 'logisticregression__penalty': 'l1', 'logisticregression__solver': 'liblinear'}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.4805852476280602\n",
      "The train F1 score is 0.4807377049180328\n",
      "The test F1 score is 0.46339501206757844\n"
     ]
    }
   ],
   "source": [
    "# For logistic regression model\n",
    "log_reg = make_pipeline(ct_ss1, LogisticRegression(max_iter=1000)) \n",
    "param_grid_log_reg  = {\"logisticregression__penalty\": ['l1', 'l2'],\n",
    "                        \"logisticregression__C\": [0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n",
    "                        \"logisticregression__solver\": ['liblinear'],\n",
    "                       }\n",
    "grid_log_reg  = GridSearchCV(log_reg, param_grid_log_reg, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_log_reg, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          transformers=[('scale',\n",
      "                                                                         StandardScaler(),\n",
      "                                                                         ['inc',\n",
      "                                                                          'age',\n",
      "                                                                          'nettfa',\n",
      "                                                                          'incsq',\n",
      "                                                                          'agesq']),\n",
      "                                                                        ('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('kneighborsclassifier',\n",
      "                                        KNeighborsClassifier(n_jobs=-1))]),\n",
      "             param_grid={'kneighborsclassifier__algorithm': ['auto',\n",
      "                                                             'ball_tree',\n",
      "                                                             'kd_tree'],\n",
      "                         'kneighborsclassifier__n_neighbors': [2, 3, 4, 5, 6, 7,\n",
      "                                                               8, 9, 10],\n",
      "                         'kneighborsclassifier__p': [1, 2]},\n",
      "             scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'kneighborsclassifier__algorithm': 'auto', 'kneighborsclassifier__n_neighbors': 3, 'kneighborsclassifier__p': 1}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.4930008337122911\n",
      "The train F1 score is 0.7304412309974045\n",
      "The test F1 score is 0.4508076358296622\n"
     ]
    }
   ],
   "source": [
    "# For KNN classification model\n",
    "knn_class = make_pipeline(ct_ss1, KNeighborsClassifier(n_jobs=-1))\n",
    "param_grid_knn_class = {\"kneighborsclassifier__n_neighbors\": [2,3,4,5,6,7,8,9,10],\n",
    "                        \"kneighborsclassifier__algorithm\": ['auto', 'ball_tree', 'kd_tree'],\n",
    "                        \"kneighborsclassifier__p\": [1, 2],\n",
    "                       }\n",
    "grid_knn_class = GridSearchCV(knn_class, param_grid_knn_class, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_knn_class, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('gaussiannb', GaussianNB())]),\n",
      "             param_grid={}, scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.4167319470332435\n",
      "The train F1 score is 0.42091092663226387\n",
      "The test F1 score is 0.40277777777777773\n"
     ]
    }
   ],
   "source": [
    "# For Gaussian Naive Bayes classification model\n",
    "gnb_class = make_pipeline(ct_no_ss1, GaussianNB())\n",
    "param_grid_gnb_class = {\n",
    "                       }\n",
    "grid_gnb_class = GridSearchCV(gnb_class, param_grid_gnb_class, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_gnb_class, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('decisiontreeclassifier',\n",
      "                                        DecisionTreeClassifier(random_state=42))]),\n",
      "             param_grid={'decisiontreeclassifier__criterion': ['gini',\n",
      "                                                               'entropy',\n",
      "                                                               'log_loss'],\n",
      "                         'decisiontreeclassifier__max_depth': [2, 3, 4, 5, 6]},\n",
      "             scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'decisiontreeclassifier__criterion': 'entropy', 'decisiontreeclassifier__max_depth': 4}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.5596340504993759\n",
      "The train F1 score is 0.5805978360535485\n",
      "The test F1 score is 0.5567606652205351\n"
     ]
    }
   ],
   "source": [
    "# For Decision Tree classification model\n",
    "dt_class = make_pipeline(ct_no_ss1, DecisionTreeClassifier(random_state=42))\n",
    "param_grid_dt_class = {\"decisiontreeclassifier__criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "                       \"decisiontreeclassifier__max_depth\": [2,3,4,5,6],\n",
    "                      }\n",
    "grid_dt_class = GridSearchCV(dt_class, param_grid_dt_class, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_dt_class, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('baggingclassifier',\n",
      "                                        BaggingClassifier(n_jobs=-1,\n",
      "                                                          random_state=42))]),\n",
      "             param_grid={'baggingclassifier__n_estimators': [10, 15, 20, 25]},\n",
      "             scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'baggingclassifier__n_estimators': 25}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.513348265566559\n",
      "The train F1 score is 0.9955032860601869\n",
      "The test F1 score is 0.5211062590975255\n"
     ]
    }
   ],
   "source": [
    "# For Bagged Decision Tree classification model\n",
    "bag_class = make_pipeline(ct_no_ss1, BaggingClassifier(n_jobs=-1, random_state=42))\n",
    "param_grid_bag_class = {\"baggingclassifier__n_estimators\": [10, 15, 20, 25],\n",
    "                       }\n",
    "grid_bag_class = GridSearchCV(bag_class, param_grid_bag_class, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_bag_class, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('extratreesclassifier',\n",
      "                                        ExtraTreesClassifier(n_jobs=-1,\n",
      "                                                             random_state=42))]),\n",
      "             param_grid={'extratreesclassifier__criterion': ['gini', 'entropy',\n",
      "                                                             'log_loss'],\n",
      "                         'extratreesclassifier__max_depth': [2, 3, 4, 5],\n",
      "                         'extratreesclassifier__n_estimators': [75, 100, 125,\n",
      "                                                                150]},\n",
      "             scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'extratreesclassifier__criterion': 'gini', 'extratreesclassifier__max_depth': 5, 'extratreesclassifier__n_estimators': 125}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.2494807167208452\n",
      "The train F1 score is 0.2786796536796537\n",
      "The test F1 score is 0.27650727650727647\n"
     ]
    }
   ],
   "source": [
    "# For Extra Trees classification model\n",
    "et_class = make_pipeline(ct_no_ss1, ExtraTreesClassifier(n_jobs=-1, random_state=42))\n",
    "param_grid_et_class = {\"extratreesclassifier__n_estimators\": [75, 100, 125, 150],\n",
    "                       \"extratreesclassifier__criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "                       \"extratreesclassifier__max_depth\": [2,3,4,5],\n",
    "                      }\n",
    "grid_et_class = GridSearchCV(et_class, param_grid_et_class, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_et_class, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('randomforestclassifier',\n",
      "                                        RandomForestClassifier(n_jobs=-1,\n",
      "                                                               random_state=42))]),\n",
      "             param_grid={'randomforestclassifier__criterion': ['gini',\n",
      "                                                               'entropy',\n",
      "                                                               'log_loss'],\n",
      "                         'randomforestclassifier__max_depth': [2, 3, 4, 5, 6, 7,\n",
      "                                                               8],\n",
      "                         'randomforestclassifier__n_estimators': [75, 100, 125,\n",
      "                                                                  150]},\n",
      "             scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'randomforestclassifier__criterion': 'gini', 'randomforestclassifier__max_depth': 8, 'randomforestclassifier__n_estimators': 75}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.5505786708480683\n",
      "The train F1 score is 0.6167247386759581\n",
      "The test F1 score is 0.531082118188795\n"
     ]
    }
   ],
   "source": [
    "# For Random Forest classification model\n",
    "rf_class = make_pipeline(ct_no_ss1, RandomForestClassifier(n_jobs=-1, random_state=42))\n",
    "param_grid_rf_class = {\"randomforestclassifier__n_estimators\": [75, 100, 125, 150],\n",
    "                       \"randomforestclassifier__criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "                       \"randomforestclassifier__max_depth\": [2,3,4,5,6,7,8],\n",
    "                      }\n",
    "grid_rf_class = GridSearchCV(rf_class, param_grid_rf_class, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_rf_class, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('adaboostclassifier',\n",
      "                                        AdaBoostClassifier(random_state=42))]),\n",
      "             param_grid={'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R'],\n",
      "                         'adaboostclassifier__n_estimators': [50, 100, 150]},\n",
      "             scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'adaboostclassifier__algorithm': 'SAMME', 'adaboostclassifier__n_estimators': 100}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.5626027794205496\n",
      "The train F1 score is 0.5683930942895086\n",
      "The test F1 score is 0.548148148148148\n"
     ]
    }
   ],
   "source": [
    "# For AdaBoost classification model\n",
    "ada_class = make_pipeline(ct_no_ss1, AdaBoostClassifier(random_state=42))\n",
    "param_grid_ada_class = {\"adaboostclassifier__n_estimators\": [50, 100, 150],\n",
    "                        \"adaboostclassifier__algorithm\": ['SAMME', 'SAMME.R'],\n",
    "                       }\n",
    "grid_ada_class = GridSearchCV(ada_class, param_grid_ada_class, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_ada_class, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          remainder='passthrough',\n",
      "                                                          transformers=[('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('gradientboostingclassifier',\n",
      "                                        GradientBoostingClassifier(random_state=42))]),\n",
      "             param_grid={'gradientboostingclassifier__criterion': ['friedman_mse',\n",
      "                                                                   'squared_error'],\n",
      "                         'gradientboostingclassifier__loss': ['log_loss',\n",
      "                                                              'exponential'],\n",
      "                         'gradientboostingclassifier__max_depth': [2, 3],\n",
      "                         'gradientboostingclassifier__n_estimators': [75, 100,\n",
      "                                                                      125,\n",
      "                                                                      150]},\n",
      "             scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'gradientboostingclassifier__criterion': 'friedman_mse', 'gradientboostingclassifier__loss': 'exponential', 'gradientboostingclassifier__max_depth': 3, 'gradientboostingclassifier__n_estimators': 75}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.5575098988627959\n",
      "The train F1 score is 0.5832203061422204\n",
      "The test F1 score is 0.5372907153729072\n"
     ]
    }
   ],
   "source": [
    "# For Gradient Boosted classification model\n",
    "gb_class = make_pipeline(ct_no_ss1, GradientBoostingClassifier(random_state=42))\n",
    "param_grid_gb_class = {\"gradientboostingclassifier__loss\": ['log_loss', 'exponential'],\n",
    "                       \"gradientboostingclassifier__n_estimators\": [75, 100, 125, 150],\n",
    "                       \"gradientboostingclassifier__criterion\": ['friedman_mse', 'squared_error'],\n",
    "                       \"gradientboostingclassifier__max_depth\": [2, 3],\n",
    "                      }\n",
    "grid_gb_class = GridSearchCV(gb_class, param_grid_gb_class, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_gb_class, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('columntransformer',\n",
      "                                        ColumnTransformer(n_jobs=-1,\n",
      "                                                          transformers=[('scale',\n",
      "                                                                         StandardScaler(),\n",
      "                                                                         ['inc',\n",
      "                                                                          'age',\n",
      "                                                                          'nettfa',\n",
      "                                                                          'incsq',\n",
      "                                                                          'agesq']),\n",
      "                                                                        ('ohe',\n",
      "                                                                         OneHotEncoder(drop='first'),\n",
      "                                                                         ['marr',\n",
      "                                                                          'male',\n",
      "                                                                          'fsize',\n",
      "                                                                          'pira'])])),\n",
      "                                       ('svc', SVC(random_state=42))]),\n",
      "             param_grid={'svc__degree': [2, 3, 4],\n",
      "                         'svc__gamma': ['scale', 'auto'],\n",
      "                         'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
      "             scoring='f1'): \n",
      "\n",
      "The best parameters as chosen by GridSearchCV is {'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'sigmoid'}\n",
      "The mean cross-validated F1 score of the best_estimator is 0.47118754246314465\n",
      "The train F1 score is 0.4179260031266285\n",
      "The test F1 score is 0.3826322930800543\n"
     ]
    }
   ],
   "source": [
    "# For Support Vector classification model\n",
    "svc = make_pipeline(ct_ss1, SVC(random_state=42))\n",
    "param_grid_svc = {\"svc__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                  \"svc__degree\": [2,3,4],\n",
    "                  \"svc__gamma\": ['scale', 'auto'],\n",
    "                 }\n",
    "grid_svc = GridSearchCV(svc, param_grid_svc, scoring='f1', cv=5)\n",
    "custom_scorer_classifier(grid_svc, X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Positives: Someone that the model incorrectly predicts is eligible for a 401(k)\n",
    "\n",
    "False Negatives: Someone that the model incorrectly predicts is not eligible for a 401(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to assume that the cost to the financial services company that I am working for is greater if they advertise/offer a 401k to someone who is not actually eligible for one than if they did not advertise/offer a 401k to someone who is eligible. Under this assumption, we would rather minimize False Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "If we wanted to minimize False Positives, we should optimize the Precision metric or Specificity (1- False Postive Rate) metric.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score might be an appropriate metric to use here because it considers both the model's precision and recall to evaluate the model's performance. The F1 score is the harmonic mean of precision and recall and can be appropriate when the average ratio is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                 Model                | Training F1 Score | Cross-validated F1 Score | Testing F1 Score |\r\n",
    "|:------------------------------------:|:-----------------:|:------------------------:|:----------------:|\r\n",
    "|          Logistic Regression         |       0.4817      |          0.4824          |      0.4621      |\r\n",
    "|         Gaussian Naive Bayes         |       0.4404      |          0.4342          |      0.4103      |\r\n",
    "|  k-Nearest Neighbors Classification  |       0.5913      |          0.4796          |      0.4612      |\r\n",
    "|     Decision Tree Classification     |       0.5699      |          0.5622          |      0.5377      |\r\n",
    "| Bagged Decision Trees Classification |       0.9953      |          0.5205          |      0.5060      |\r\n",
    "|      Extra Trees Classification      |       0.2961      |          0.2777          |      0.2642      |\r\n",
    "|     Random Forest Classification     |       0.6026      |          0.5504          |      0.5278      |\r\n",
    "|        AdaBoost Classification       |       0.5696      |          0.5635          |      0.5404      |\r\n",
    "|    Gradient Boosted Classification   |       0.5858      |          0.5589          |      0.5437      |\r\n",
    "|     Support Vector Classification    |       0.4649      |          0.4682          |      0.4639      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training f1-score and the testing f1-score, there is evidence of more overfitting, in this order:\n",
    "\n",
    "1. Bagged Decision Trees Classification: Difference in f1-score of about 0.49\n",
    "2. k-Nearest Neighbors Classification: Difference in f1-score of about 0.13\n",
    "3. Random Forest Classification: Difference in f1-score of about 0.075\n",
    "4. The rest have differences in f1-score of about 0.04 or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I have a series of models from which I can pick, I usually do the following:\n",
    "\n",
    "1. List out all of the models. Any model that cannot solve my problem should be removed. The problem statement is \"Predict whether or not one is eligible for a 401k?\" I want to find the model that performs the best based on a metric of my choice, in this case the f1-score\n",
    "2. The model with the best test f1-score is the Gradient Boosted Classification, although it is slightly more overfitted than the AdaBoost Classification, as based on difference in train/test f1-score. Therefore, I have decided to pick the **AdaBoost Classification** instead. Regarding interpretability of my selected model (although not a requirement):\n",
    "    - For decision tree ensembles, we can simply average the impurity-based feature importance of each tree to determine the importance of features. I have shown the code below to show it can be done, similar as to for the regression problem statement \n",
    "3. Sometimes, model selection becomes a judgment call with no perfect guide to make the final decision\n",
    "    - Do you have time to tune the models to try and eke out better performance?\n",
    "    - Is one model substantially better at solving the problem you wanted to solve?\n",
    "    - Do you need something understandable by a lay audience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. I would look at higher-order terms. For example, since we have incsq and agesq automatically created for us, there may be other higher-order terms that could be predictive. I would explore these to see if we can make better predictions. This recommendation was included above but still applies for classification problems\n",
    "2. I would also consider interaction terms between variables. For example between income and nettfa. This recommendation was included above but still applies for classification problems\n",
    "3. I would consider transforming my variable. Income is often skewed, which means there will usually be a handful of very high incomes that might skew any linear model. I would consider transforming income (likely using log) so that income is \"un-skewed\". This recommendation was included above but still applies for classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting coefficient values for our chosen classification model\n",
    "grid_ada_class.fit(X_train1, y_train1) \n",
    "ada_class.set_params(**grid_ada_class.best_params_).fit(X_train1, y_train1)\n",
    "feature_values_class = ada_class['adaboostclassifier'].feature_importances_\n",
    "feature_names_class = ada_class.named_steps.columntransformer.get_feature_names_out()\n",
    "classification_coefficients = pd.DataFrame(feature_values_class, index=feature_names_class).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ohe__marr_1</th>\n",
       "      <th>ohe__male_1</th>\n",
       "      <th>ohe__fsize_2</th>\n",
       "      <th>ohe__fsize_3</th>\n",
       "      <th>ohe__fsize_4</th>\n",
       "      <th>ohe__fsize_5</th>\n",
       "      <th>ohe__fsize_6</th>\n",
       "      <th>ohe__fsize_7</th>\n",
       "      <th>ohe__fsize_8</th>\n",
       "      <th>ohe__fsize_9</th>\n",
       "      <th>ohe__fsize_10</th>\n",
       "      <th>ohe__fsize_11</th>\n",
       "      <th>ohe__fsize_12</th>\n",
       "      <th>ohe__fsize_13</th>\n",
       "      <th>remainder__age</th>\n",
       "      <th>remainder__nettfa</th>\n",
       "      <th>remainder__agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.213725</td>\n",
       "      <td>0.010224</td>\n",
       "      <td>0.00543</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026327</td>\n",
       "      <td>0.705554</td>\n",
       "      <td>0.029098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ohe__marr_1  ohe__male_1  ohe__fsize_2  ohe__fsize_3  ohe__fsize_4  \\\n",
       "0     0.213725     0.010224       0.00543      0.000457      0.003655   \n",
       "\n",
       "   ohe__fsize_5  ohe__fsize_6  ohe__fsize_7  ohe__fsize_8  ohe__fsize_9  \\\n",
       "0      0.001046      0.001562      0.001359      0.000555      0.000607   \n",
       "\n",
       "   ohe__fsize_10  ohe__fsize_11  ohe__fsize_12  ohe__fsize_13  remainder__age  \\\n",
       "0       0.000401            0.0            0.0            0.0        0.026327   \n",
       "\n",
       "   remainder__nettfa  remainder__agesq  \n",
       "0           0.705554          0.029098  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print regression coefficients\n",
    "regression_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettfa is the most important predictor of income, with a value of 0.7055. Every increase in net total financial assets of one unit ($1000) increases inc by 0.7055 (the data dictionary is not clear on the units for income, but perhaps it is in 1000s also). Being married (1) is the second most important feature with married persons having a greater inc of 0.2137 compared to unmarried (0) persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ohe__marr_1</th>\n",
       "      <th>ohe__male_1</th>\n",
       "      <th>ohe__fsize_2</th>\n",
       "      <th>ohe__fsize_3</th>\n",
       "      <th>ohe__fsize_4</th>\n",
       "      <th>ohe__fsize_5</th>\n",
       "      <th>ohe__fsize_6</th>\n",
       "      <th>ohe__fsize_7</th>\n",
       "      <th>ohe__fsize_8</th>\n",
       "      <th>ohe__fsize_9</th>\n",
       "      <th>ohe__fsize_10</th>\n",
       "      <th>ohe__fsize_11</th>\n",
       "      <th>ohe__fsize_12</th>\n",
       "      <th>ohe__fsize_13</th>\n",
       "      <th>ohe__pira_1</th>\n",
       "      <th>remainder__inc</th>\n",
       "      <th>remainder__age</th>\n",
       "      <th>remainder__nettfa</th>\n",
       "      <th>remainder__incsq</th>\n",
       "      <th>remainder__agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011853</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009903</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074807</td>\n",
       "      <td>0.090443</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.328524</td>\n",
       "      <td>0.294227</td>\n",
       "      <td>0.096085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ohe__marr_1  ohe__male_1  ohe__fsize_2  ohe__fsize_3  ohe__fsize_4  \\\n",
       "0     0.011853     0.007381           0.0           0.0      0.010278   \n",
       "\n",
       "   ohe__fsize_5  ohe__fsize_6  ohe__fsize_7  ohe__fsize_8  ohe__fsize_9  \\\n",
       "0           0.0           0.0      0.009903      0.036893           0.0   \n",
       "\n",
       "   ohe__fsize_10  ohe__fsize_11  ohe__fsize_12  ohe__fsize_13  ohe__pira_1  \\\n",
       "0            0.0            0.0            0.0            0.0     0.074807   \n",
       "\n",
       "   remainder__inc  remainder__age  remainder__nettfa  remainder__incsq  \\\n",
       "0        0.090443        0.039604           0.328524          0.294227   \n",
       "\n",
       "   remainder__agesq  \n",
       "0          0.096085  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print classification coefficients\n",
    "classification_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict whether someone is eligible or not for a 401k with a f1-score of 0.5404. The coefficients are not necessary for the problem statement but I just wanted to show it can be done. The most important feature used for spltting is nettfa followed by incsq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
